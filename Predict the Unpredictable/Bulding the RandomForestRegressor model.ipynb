{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive to access files\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWYd1YcHp4N9",
        "outputId": "711435de-e7be-4ebd-cf59-992bfe264ea7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**statistics**"
      ],
      "metadata": {
        "id": "aYgkJqqJqCjI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> **Bulding Model**\n",
        "\n"
      ],
      "metadata": {
        "id": "-LBdm7OzpiQ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1. Load and preprocess the data\n",
        "def load_and_preprocess_data(file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "    df['date'] = pd.to_datetime(df['date'])\n",
        "    df['year'] = df['date'].dt.year\n",
        "    df['month'] = df['date'].dt.month\n",
        "    df['day'] = df['date'].dt.day\n",
        "    df['dayofyear'] = df['date'].dt.dayofyear\n",
        "    return df\n",
        "\n",
        "# load the file path\n",
        "historical_data = load_and_preprocess_data('/content/drive/MyDrive/historical_weather.csv')\n",
        "\n",
        "# 2. Feature engineering\n",
        "def create_features(df):\n",
        "    df['temp_1d_lag'] = df.groupby('city_id')['avg_temp_c'].shift(1)\n",
        "    df['temp_1w_lag'] = df.groupby('city_id')['avg_temp_c'].shift(7)\n",
        "    df['temp_1m_lag'] = df.groupby('city_id')['avg_temp_c'].shift(30)\n",
        "    df['temp_1y_lag'] = df.groupby('city_id')['avg_temp_c'].shift(365)\n",
        "\n",
        "    # Create rolling averages\n",
        "    df['temp_7d_avg'] = df.groupby('city_id')['avg_temp_c'].rolling(window=7).mean().reset_index(0, drop=True)\n",
        "    df['temp_30d_avg'] = df.groupby('city_id')['avg_temp_c'].rolling(window=30).mean().reset_index(0, drop=True)\n",
        "\n",
        "    return df\n",
        "\n",
        "historical_data = create_features(historical_data)\n",
        "\n",
        "# 3. Prepare the data for modeling\n",
        "features = ['year', 'month', 'day', 'dayofyear', 'temp_1d_lag', 'temp_1w_lag', 'temp_1m_lag', 'temp_1y_lag', 'temp_7d_avg', 'temp_30d_avg']\n",
        "X = historical_data[features].dropna()\n",
        "y = historical_data.loc[X.index, 'avg_temp_c']\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# 4. Train the model\n",
        "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# 5. Evaluate the model\n",
        "train_predictions = model.predict(X_train_scaled)\n",
        "test_predictions = model.predict(X_test_scaled)\n",
        "\n",
        "train_rmse = np.sqrt(mean_squared_error(y_train, train_predictions))\n",
        "test_rmse = np.sqrt(mean_squared_error(y_test, test_predictions))\n",
        "\n",
        "print(f\"Train RMSE: {train_rmse}\")\n",
        "print(f\"Test RMSE: {test_rmse}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9TVQbG2fYHMx",
        "outputId": "fe935d9c-f803-47e5-d420-6137fefa58e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train RMSE: 0.655091459516315\n",
            "Test RMSE: 1.734284941685503\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Get Predictions**"
      ],
      "metadata": {
        "id": "kgx1Omovpo7v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Generate predictions for the submission period\n",
        "def generate_submission_data(model, scaler, submission_key_path):\n",
        "    # Load the submission key\n",
        "    submission_key = pd.read_csv(submission_key_path)\n",
        "    submission_key['date'] = pd.to_datetime(submission_key['date'])\n",
        "\n",
        "    # Create features for the submission period\n",
        "    submission_key['year'] = submission_key['date'].dt.year\n",
        "    submission_key['month'] = submission_key['date'].dt.month\n",
        "    submission_key['day'] = submission_key['date'].dt.day\n",
        "    submission_key['dayofyear'] = submission_key['date'].dt.dayofyear\n",
        "\n",
        "    # We need to get the lag features from the historical data\n",
        "    # This assumes that the historical data covers the period just before the submission period\n",
        "    last_historical_data = historical_data.groupby('city_id').last().reset_index()\n",
        "\n",
        "    submission_data = pd.merge(submission_key, last_historical_data[['city_id', 'avg_temp_c', 'temp_7d_avg', 'temp_30d_avg']],\n",
        "                               on='city_id', how='left')\n",
        "\n",
        "    submission_data['temp_1d_lag'] = submission_data['avg_temp_c']\n",
        "    submission_data['temp_1w_lag'] = submission_data['avg_temp_c']\n",
        "    submission_data['temp_1m_lag'] = submission_data['avg_temp_c']\n",
        "    submission_data['temp_1y_lag'] = submission_data['avg_temp_c']\n",
        "\n",
        "    # Prepare features for prediction\n",
        "    features = ['year', 'month', 'day', 'dayofyear', 'temp_1d_lag', 'temp_1w_lag', 'temp_1m_lag', 'temp_1y_lag', 'temp_7d_avg', 'temp_30d_avg']\n",
        "    X_submission = submission_data[features]\n",
        "    X_submission_scaled = scaler.transform(X_submission)\n",
        "\n",
        "    # Make predictions\n",
        "    predictions = model.predict(X_submission_scaled)\n",
        "\n",
        "    # Create submission dataframe\n",
        "    submission = pd.DataFrame({\n",
        "        'submission_ID': submission_key['submission_ID'],\n",
        "        'avg_temp_c': predictions\n",
        "    })\n",
        "\n",
        "    return submission\n",
        "\n",
        "# Generate submission\n",
        "submission_key_path = '/content/drive/MyDrive/submission_key.csv'\n",
        "submission = generate_submission_data(model, scaler, submission_key_path)\n",
        "\n",
        "# Save submission to CSV\n",
        "submission.to_csv('/content/drive/MyDrive/submission.csv', index=False)\n",
        "\n",
        "print(\"Submission file created successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "urbOaAX9PvSM",
        "outputId": "735d8506-0b23-41a7-ec43-1f7d8ca8e8cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Submission file created successfully.\n"
          ]
        }
      ]
    }
  ]
}