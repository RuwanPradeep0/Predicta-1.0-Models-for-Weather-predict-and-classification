# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18taA5FAmtlAmdSFqz6TCcnma17AJGTYv

**Data Preprocessing**
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.model_selection import train_test_split

# Load the dataset
file_path = '/content/daily_data.csv'
data = pd.read_csv(file_path)
data.head()

data.info()

# Visualize missing values
plt.figure(figsize=(12, 8))
sns.heatmap(data.isnull(), cbar=False, cmap='viridis')
plt.title('Missing Values in Dataset')
plt.show()

# Handle missing values
train_data = data.dropna(subset=['condition_text'])
test_data = data[data['condition_text'].isnull()]

train_data.info()
test_data.info()

from sklearn.preprocessing import LabelEncoder

# Assuming train_data and label_encoder are defined
label_encoder = LabelEncoder()

# Encode 'condition_text' and assign to a new column 'condition_text_encoded'
train_data.loc[:, 'condition_text_encoded'] = label_encoder.fit_transform(train_data['condition_text'])

# Drop non-numeric and unnecessary columns
columns_to_drop = ['day_id', 'city_id', 'condition_text', 'sunrise', 'sunset']
X = train_data.drop(columns=columns_to_drop + ['condition_text_encoded'])
y = train_data['condition_text_encoded']

# Handle missing values in features
imputer = SimpleImputer(strategy='mean')
X = imputer.fit_transform(X)

# Standardize the features
scaler = StandardScaler()
X = scaler.fit_transform(X)

# Visualize the distributions of some features before and after scaling
fig, axs = plt.subplots(2, 2, figsize=(14, 10))
sns.histplot(train_data['temperature_celsius'], kde=True, ax=axs[0, 0]).set_title('Original Temperature Distribution')
sns.histplot(X[:, 0], kde=True, ax=axs[0, 1]).set_title('Scaled Temperature Distribution')
sns.histplot(train_data['wind_kph'], kde=True, ax=axs[1, 0]).set_title('Original Wind Speed Distribution')
sns.histplot(X[:, 1], kde=True, ax=axs[1, 1]).set_title('Scaled Wind Speed Distribution')
plt.tight_layout()
plt.show()

"""**Feature Engineering**"""

from sklearn.preprocessing import PolynomialFeatures

# Create polynomial features
poly = PolynomialFeatures(degree=2, interaction_only=True)
X_poly = poly.fit_transform(X)

# Visualize correlation matrix of original features
plt.figure(figsize=(12, 8))
sns.heatmap(pd.DataFrame(X).corr(), annot=True, cmap='coolwarm', fmt='.2f')
plt.title('Correlation Matrix of Original Features')
plt.show()

# Visualize correlation matrix of polynomial features
plt.figure(figsize=(12, 8))
sns.heatmap(pd.DataFrame(X_poly).corr(), annot=True, cmap='coolwarm', fmt='.2f')
plt.title('Correlation Matrix of Polynomial Features')
plt.show()

pip install optuna

pip install lightgbm

import optuna
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score
from sklearn.ensemble import StackingClassifier

# Define the objective function for Bayesian Optimization
def objective(trial):
    classifier_name = trial.suggest_categorical('classifier', ['XGB', 'LGB', 'MLP'])
    if classifier_name == 'XGB':
        xgb_param = {
            'n_estimators': trial.suggest_int('n_estimators', 100, 1000),
            'max_depth': trial.suggest_int('max_depth', 3, 10),
            'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),
            'subsample': trial.suggest_uniform('subsample', 0.5, 1.0),
            'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.5, 1.0),
        }
        model = XGBClassifier(**xgb_param, random_state=42)
    elif classifier_name == 'LGB':
        lgb_param = {
            'n_estimators': trial.suggest_int('n_estimators', 100, 1000),
            'num_leaves': trial.suggest_int('num_leaves', 20, 150),
            'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),
            'subsample': trial.suggest_uniform('subsample', 0.5, 1.0),
            'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.5, 1.0),
        }
        model = LGBMClassifier(**lgb_param, random_state=42)
    else:
        mlp_param = {
            'hidden_layer_sizes': trial.suggest_categorical('hidden_layer_sizes', [(100,), (100, 50), (100, 50, 25)]),
            'activation': trial.suggest_categorical('activation', ['relu', 'tanh']),
            'learning_rate_init': trial.suggest_loguniform('learning_rate_init', 0.001, 0.01),
            'max_iter': trial.suggest_int('max_iter', 200, 1000)
        }
        model = MLPClassifier(**mlp_param, random_state=42)

    model.fit(X_train, y_train)
    y_pred = model.predict(X_val)
    accuracy = accuracy_score(y_val, y_pred)
    return accuracy

from sklearn.model_selection import train_test_split


# Split the data into training and validation sets
X_train, X_val, y_train, y_val = train_test_split(X_poly, y, test_size=0.2, random_state=42)

# Perform Bayesian Optimization
study = optuna.create_study(direction='maximize')
study.optimize(objective, n_trials=100)

# Get the best model
best_model_params = study.best_trial.params
if best_model_params['classifier'] == 'XGB':
    best_model = XGBClassifier(**{k: v for k, v in best_model_params.items() if k != 'classifier'}, random_state=42)
elif best_model_params['classifier'] == 'LGB':
    best_model = LGBMClassifier(**{k: v for k, v in best_model_params.items() if k != 'classifier'}, random_state=42)
else:
    best_model = MLPClassifier(**{k: v for k, v in best_model_params.items() if k != 'classifier'}, random_state=42)

best_model.fit(X_train, y_train)

# # Visualize the optimization results
# optuna.visualization.plot_optimization_history(study)
# optuna.visualization.plot_param_importances(study)
# plt.title("qwe")
# plt.show()


# Visualize the optimization results
import matplotlib.pyplot as plt

fig = optuna.visualization.plot_optimization_history(study)
fig.show()  # Use this instead of plt.show() for Optuna visualizations

fig = optuna.visualization.plot_param_importances(study)
fig.update_layout(title_text="Hyperparameter Importances")
fig.show()

# If you want to add a title to the last plot
plt.title("Parameter Importances")
plt.tight_layout()
plt.show()

"""**Model Stacking**"""

# Define base models for stacking
xgb_params = {k: v for k, v in study.best_trial.params.items() if 'XGB' in k}
lgb_params = {k: v for k, v in study.best_trial.params.items() if 'LGB' in k}
mlp_params = {k: v for k, v in study.best_trial.params.items() if 'MLP' in k}

base_models = [
    ('xgb', XGBClassifier(**xgb_params, random_state=42)),
    ('lgb', LGBMClassifier(**lgb_params, random_state=42)),
    ('mlp', MLPClassifier(**mlp_params, random_state=42))
]

# Define the stacking classifier
stacking_clf = StackingClassifier(estimators=base_models, final_estimator=LGBMClassifier(random_state=42))

# Train the stacking classifier
stacking_clf.fit(X_train, y_train)

# Predict on the validation set
y_val_pred = stacking_clf.predict(X_val)

# Evaluate the model
accuracy = accuracy_score(y_val, y_val_pred)
print(f'Validation Accuracy: {accuracy}')

# Confusion matrix visualization
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

cm = confusion_matrix(y_val, y_val_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=label_encoder.classes_)
disp.plot(cmap='viridis', xticks_rotation='vertical')
plt.title('Confusion Matrix')
plt.show()

# Prepare the test data
X_test = test_data.drop(columns=columns_to_drop)
X_test = imputer.transform(X_test)
X_test = scaler.transform(X_test)
X_test_poly = poly.transform(X_test)

# Predict the missing condition_text values
test_data.loc[:, 'condition_text_pred'] = stacking_clf.predict(X_test_poly)
test_data.loc[:, 'condition_text'] = label_encoder.inverse_transform(test_data['condition_text_pred'])

# Prepare the submission file
submission = test_data[['day_id', 'condition_text']]
submission.to_csv('/content/submission.csv', index=False)

# Visualize the predicted distribution
plt.figure(figsize=(10, 6))
sns.countplot(x='condition_text', data=test_data)
plt.xticks(rotation=90)
plt.title('Distribution of Predicted Weather Conditions')
plt.show()